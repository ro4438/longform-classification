{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import torch\n",
    "from transformers import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import pathlib\n",
    "\n",
    "#output all items, not just last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#set device\n",
    "if torch.cuda.is_available():\n",
    "    device= \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"  \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Articles dataset class for easy sampling, iteration, and weight creating\n",
    "class Articles(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        super().__init__()\n",
    "        with open(json_file, \"r\") as data_file:\n",
    "            self.examples = json.loads(data_file.read())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def tokenize(self):\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            self.examples[idx]['text'] = re.findall('[\\w]+', self.examples[idx]['text'].lower())\n",
    "\n",
    "    def create_positive_sampler(self, target_publication):\n",
    "        prob = np.zeros(len(self))\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            if example['model_publication'] == target_publication:\n",
    "                prob[idx] = 1\n",
    "        return torch.utils.data.WeightedRandomSampler(weights=prob, num_samples=len(self), replacement=True)\n",
    "\n",
    "    def create_negative_sampler(self, target_publication):\n",
    "        prob = np.zeros(len(self))\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            if example['model_publication'] != target_publication:\n",
    "                prob[idx] = 1\n",
    "        return torch.utils.data.WeightedRandomSampler(weights=prob, num_samples=len(self), replacement=True)\n",
    "\n",
    "    def map_items(self, word_to_id, url_to_id, publication_to_id, filter=False, min_length=0):\n",
    "        min_length_articles = []\n",
    "        for idx, example in enumerate(self.examples):\n",
    "            self.examples[idx]['text'] = [word_to_id.get(word, len(word_to_id)) for word in example['text']]\n",
    "            self.examples[idx]['text'] = [word for word in example['text'] if word != len(word_to_id)]\n",
    "            if filter:\n",
    "                if len(self.examples[idx]['text']) > min_length:\n",
    "                    min_length_articles.append(self.examples[idx])\n",
    "            self.examples[idx]['url'] = url_to_id.get(example['url'], url_to_id.get(\"miscellaneous\"))\n",
    "            self.examples[idx]['model_publication'] = publication_to_id.get(example['model_publication'], publication_to_id.get(\"miscellaneous\"))\n",
    "        return min_length_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Longformer tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading Longformer tokenizer...')\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  The White House medical unit and Secret Service will evaluate attendees before they're admitted. They'll be required to test negative for the virus on the day of the event, complete a health questionnaire and pass a temperature screening. The sites will be cleaned and sanitized before each event. Trump Victory, the president’s joint fundraising committee, will cover the testing costs. The move comes as Trump expresses his desire for the country to reopen even as medical professionals raise concerns about the potential dangers of doing so. The pandemic death toll passed 100,000 this week. Trump is also dead set on holding the GOP’s August national convention in Charlotte, even as North Carolina officials are raising concerns about safety. The Charlotte area has seen an uptick in coronavirus cases in recent days. Trump has also been itching to resume his trademark rallies, his primary method of connecting with supporters and broadcasting his message. The president’s campaign team has used the events to glean data from attendees which is used to turn out his voters. The president has left the confines of the White House over the past few weeks to hold ostensibly official events in swing states like Arizona and Michigan, where polls have shown him trailing presumptive Democratic nominee Joe Biden. The events have sometimes had the feel of a rally, complete with walk-out music. Trump’s advisers want him to be seen as eager to reopen the country while Democrats push for stay-at-home orders that keep the economy shuttered. Earlier this month, the reelection effort released a one-minute advertisement titled “American Comeback” highlighting the president’s desire to reignite the economy. The spot concluded with Trump’s mantra that he will “make America great again.” The president’s previously busy fundraising schedule came to a halt in March. A planned March 12 fundraiser with GOP megadonor Sheldon Adelson was scrapped, as were a pair of events that month to be headlined by first lady Melania Trump. Even with in-person fundraising events slashed, the president has maintained robust fundraising totals thanks to a massive small-donor operation. Trump’s political machine narrowly outraised Biden in April and has a $187 million cash-on-hand lead over Biden and the Democratic Party. Biden has sworn off in-person fundraisers during the pandemic, instead doing online events from his Delaware home. Couples will need to donate $580,600 to attend the Dallas fundraiser. A single attendee to the New Jersey fundraiser will need to give $250,000. The money will go to Trump Victory, a joint fundraising committee of the Trump campaign, Republican National Committee, and state parties.\n"
     ]
    }
   ],
   "source": [
    "sentences = \"The White House medical unit and Secret Service will evaluate attendees before they're admitted. They'll be required to test negative for the virus on the day of the event, complete a health questionnaire and pass a temperature screening. The sites will be cleaned and sanitized before each event. Trump Victory, the president’s joint fundraising committee, will cover the testing costs. The move comes as Trump expresses his desire for the country to reopen even as medical professionals raise concerns about the potential dangers of doing so. The pandemic death toll passed 100,000 this week. Trump is also dead set on holding the GOP’s August national convention in Charlotte, even as North Carolina officials are raising concerns about safety. The Charlotte area has seen an uptick in coronavirus cases in recent days. Trump has also been itching to resume his trademark rallies, his primary method of connecting with supporters and broadcasting his message. The president’s campaign team has used the events to glean data from attendees which is used to turn out his voters. The president has left the confines of the White House over the past few weeks to hold ostensibly official events in swing states like Arizona and Michigan, where polls have shown him trailing presumptive Democratic nominee Joe Biden. The events have sometimes had the feel of a rally, complete with walk-out music. Trump’s advisers want him to be seen as eager to reopen the country while Democrats push for stay-at-home orders that keep the economy shuttered. Earlier this month, the reelection effort released a one-minute advertisement titled “American Comeback” highlighting the president’s desire to reignite the economy. The spot concluded with Trump’s mantra that he will “make America great again.” The president’s previously busy fundraising schedule came to a halt in March. A planned March 12 fundraiser with GOP megadonor Sheldon Adelson was scrapped, as were a pair of events that month to be headlined by first lady Melania Trump. Even with in-person fundraising events slashed, the president has maintained robust fundraising totals thanks to a massive small-donor operation. Trump’s political machine narrowly outraised Biden in April and has a $187 million cash-on-hand lead over Biden and the Democratic Party. Biden has sworn off in-person fundraisers during the pandemic, instead doing online events from his Delaware home. Couples will need to donate $580,600 to attend the Dallas fundraiser. A single attendee to the New Jersey fundraiser will need to give $250,000. The money will go to Trump Victory, a joint fundraising committee of the Trump campaign, Republican National Committee, and state parties.\"\n",
    "print(' Original: ', sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:  ['The', 'ĠWhite', 'ĠHouse', 'Ġmedical', 'Ġunit', 'Ġand', 'ĠSecret', 'ĠService', 'Ġwill', 'Ġevaluate', 'Ġattendees', 'Ġbefore', 'Ġthey', \"'re\", 'Ġadmitted', '.', 'ĠThey', \"'ll\", 'Ġbe', 'Ġrequired', 'Ġto', 'Ġtest', 'Ġnegative', 'Ġfor', 'Ġthe', 'Ġvirus', 'Ġon', 'Ġthe', 'Ġday', 'Ġof', 'Ġthe', 'Ġevent', ',', 'Ġcomplete', 'Ġa', 'Ġhealth', 'Ġquestionnaire', 'Ġand', 'Ġpass', 'Ġa', 'Ġtemperature', 'Ġscreening', '.', 'ĠThe', 'Ġsites', 'Ġwill', 'Ġbe', 'Ġcleaned', 'Ġand', 'Ġsan', 'itized', 'Ġbefore', 'Ġeach', 'Ġevent', '.', 'ĠTrump', 'ĠVictory', ',', 'Ġthe', 'Ġpresident', 'âĢ', 'Ļ', 's', 'Ġjoint', 'Ġfundraising', 'Ġcommittee', ',', 'Ġwill', 'Ġcover', 'Ġthe', 'Ġtesting', 'Ġcosts', '.', 'ĠThe', 'Ġmove', 'Ġcomes', 'Ġas', 'ĠTrump', 'Ġexpresses', 'Ġhis', 'Ġdesire', 'Ġfor', 'Ġthe', 'Ġcountry', 'Ġto', 'Ġreopen', 'Ġeven', 'Ġas', 'Ġmedical', 'Ġprofessionals', 'Ġraise', 'Ġconcerns', 'Ġabout', 'Ġthe', 'Ġpotential', 'Ġdangers', 'Ġof', 'Ġdoing', 'Ġso', '.', 'ĠThe', 'Ġpand', 'emic', 'Ġdeath', 'Ġtoll', 'Ġpassed', 'Ġ100', ',', '000', 'Ġthis', 'Ġweek', '.', 'ĠTrump', 'Ġis', 'Ġalso', 'Ġdead', 'Ġset', 'Ġon', 'Ġholding', 'Ġthe', 'ĠGOP', 'âĢ', 'Ļ', 's', 'ĠAugust', 'Ġnational', 'Ġconvention', 'Ġin', 'ĠCharlotte', ',', 'Ġeven', 'Ġas', 'ĠNorth', 'ĠCarolina', 'Ġofficials', 'Ġare', 'Ġraising', 'Ġconcerns', 'Ġabout', 'Ġsafety', '.', 'ĠThe', 'ĠCharlotte', 'Ġarea', 'Ġhas', 'Ġseen', 'Ġan', 'Ġuptick', 'Ġin', 'Ġcoron', 'av', 'irus', 'Ġcases', 'Ġin', 'Ġrecent', 'Ġdays', '.', 'ĠTrump', 'Ġhas', 'Ġalso', 'Ġbeen', 'Ġitching', 'Ġto', 'Ġresume', 'Ġhis', 'Ġtrademark', 'Ġrallies', ',', 'Ġhis', 'Ġprimary', 'Ġmethod', 'Ġof', 'Ġconnecting', 'Ġwith', 'Ġsupporters', 'Ġand', 'Ġbroadcasting', 'Ġhis', 'Ġmessage', '.', 'ĠThe', 'Ġpresident', 'âĢ', 'Ļ', 's', 'Ġcampaign', 'Ġteam', 'Ġhas', 'Ġused', 'Ġthe', 'Ġevents', 'Ġto', 'Ġglean', 'Ġdata', 'Ġfrom', 'Ġattendees', 'Ġwhich', 'Ġis', 'Ġused', 'Ġto', 'Ġturn', 'Ġout', 'Ġhis', 'Ġvoters', '.', 'ĠThe', 'Ġpresident', 'Ġhas', 'Ġleft', 'Ġthe', 'Ġconfines', 'Ġof', 'Ġthe', 'ĠWhite', 'ĠHouse', 'Ġover', 'Ġthe', 'Ġpast', 'Ġfew', 'Ġweeks', 'Ġto', 'Ġhold', 'Ġostensibly', 'Ġofficial', 'Ġevents', 'Ġin', 'Ġswing', 'Ġstates', 'Ġlike', 'ĠArizona', 'Ġand', 'ĠMichigan', ',', 'Ġwhere', 'Ġpolls', 'Ġhave', 'Ġshown', 'Ġhim', 'Ġtrailing', 'Ġpresumptive', 'ĠDemocratic', 'Ġnominee', 'ĠJoe', 'ĠBiden', '.', 'ĠThe', 'Ġevents', 'Ġhave', 'Ġsometimes', 'Ġhad', 'Ġthe', 'Ġfeel', 'Ġof', 'Ġa', 'Ġrally', ',', 'Ġcomplete', 'Ġwith', 'Ġwalk', '-', 'out', 'Ġmusic', '.', 'ĠTrump', 'âĢ', 'Ļ', 's', 'Ġadvisers', 'Ġwant', 'Ġhim', 'Ġto', 'Ġbe', 'Ġseen', 'Ġas', 'Ġeager', 'Ġto', 'Ġreopen', 'Ġthe', 'Ġcountry', 'Ġwhile', 'ĠDemocrats', 'Ġpush', 'Ġfor', 'Ġstay', '-', 'at', '-', 'home', 'Ġorders', 'Ġthat', 'Ġkeep', 'Ġthe', 'Ġeconomy', 'Ġshut', 'tered', '.', 'ĠEarlier', 'Ġthis', 'Ġmonth', ',', 'Ġthe', 'Ġreelection', 'Ġeffort', 'Ġreleased', 'Ġa', 'Ġone', '-', 'minute', 'Ġadvertisement', 'Ġtitled', 'ĠâĢ', 'ľ', 'American', 'ĠCome', 'back', 'âĢ', 'Ŀ', 'Ġhighlighting', 'Ġthe', 'Ġpresident', 'âĢ', 'Ļ', 's', 'Ġdesire', 'Ġto', 'Ġreign', 'ite', 'Ġthe', 'Ġeconomy', '.', 'ĠThe', 'Ġspot', 'Ġconcluded', 'Ġwith', 'ĠTrump', 'âĢ', 'Ļ', 's', 'Ġmantra', 'Ġthat', 'Ġhe', 'Ġwill', 'ĠâĢ', 'ľ', 'make', 'ĠAmerica', 'Ġgreat', 'Ġagain', '.', 'âĢ', 'Ŀ', 'ĠThe', 'Ġpresident', 'âĢ', 'Ļ', 's', 'Ġpreviously', 'Ġbusy', 'Ġfundraising', 'Ġschedule', 'Ġcame', 'Ġto', 'Ġa', 'Ġhalt', 'Ġin', 'ĠMarch', '.', 'ĠA', 'Ġplanned', 'ĠMarch', 'Ġ12', 'Ġfundraiser', 'Ġwith', 'ĠGOP', 'Ġmeg', 'ad', 'on', 'or', 'ĠSheldon', 'ĠAd', 'elson', 'Ġwas', 'Ġscrapped', ',', 'Ġas', 'Ġwere', 'Ġa', 'Ġpair', 'Ġof', 'Ġevents', 'Ġthat', 'Ġmonth', 'Ġto', 'Ġbe', 'Ġheadlined', 'Ġby', 'Ġfirst', 'Ġlady', 'ĠMelania', 'ĠTrump', '.', 'ĠEven', 'Ġwith', 'Ġin', '-', 'person', 'Ġfundraising', 'Ġevents', 'Ġslashed', ',', 'Ġthe', 'Ġpresident', 'Ġhas', 'Ġmaintained', 'Ġrobust', 'Ġfundraising', 'Ġtotals', 'Ġthanks', 'Ġto', 'Ġa', 'Ġmassive', 'Ġsmall', '-', 'don', 'or', 'Ġoperation', '.', 'ĠTrump', 'âĢ', 'Ļ', 's', 'Ġpolitical', 'Ġmachine', 'Ġnarrowly', 'Ġout', 'raised', 'ĠBiden', 'Ġin', 'ĠApril', 'Ġand', 'Ġhas', 'Ġa', 'Ġ$', '187', 'Ġmillion', 'Ġcash', '-', 'on', '-', 'hand', 'Ġlead', 'Ġover', 'ĠBiden', 'Ġand', 'Ġthe', 'ĠDemocratic', 'ĠParty', '.', 'ĠBiden', 'Ġhas', 'Ġsworn', 'Ġoff', 'Ġin', '-', 'person', 'Ġfundra', 'isers', 'Ġduring', 'Ġthe', 'Ġpand', 'emic', ',', 'Ġinstead', 'Ġdoing', 'Ġonline', 'Ġevents', 'Ġfrom', 'Ġhis', 'ĠDelaware', 'Ġhome', '.', 'ĠCou', 'ples', 'Ġwill', 'Ġneed', 'Ġto', 'Ġdonate', 'Ġ$', '580', ',', '600', 'Ġto', 'Ġattend', 'Ġthe', 'ĠDallas', 'Ġfundraiser', '.', 'ĠA', 'Ġsingle', 'Ġattend', 'ee', 'Ġto', 'Ġthe', 'ĠNew', 'ĠJersey', 'Ġfundraiser', 'Ġwill', 'Ġneed', 'Ġto', 'Ġgive', 'Ġ$', '250', ',', '000', '.', 'ĠThe', 'Ġmoney', 'Ġwill', 'Ġgo', 'Ġto', 'ĠTrump', 'ĠVictory', ',', 'Ġa', 'Ġjoint', 'Ġfundraising', 'Ġcommittee', 'Ġof', 'Ġthe', 'ĠTrump', 'Ġcampaign', ',', 'ĠRepublican', 'ĠNational', 'ĠCommittee', ',', 'Ġand', 'Ġstate', 'Ġparties', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LongformerTokenizer' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8a6f7c5739ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LongformerTokenizer' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "for idx, token in enumerate(tokenizer.vocab.keys()):\n",
    "    print(token, \" : \", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:  [133, 735, 446, 1131, 1933, 8, 8350, 1841, 40, 10516, 10703, 137, 51, 214, 2641, 4, 252, 581, 28, 1552, 7, 1296, 2430, 13, 5, 6793, 15, 5, 183, 9, 5, 515, 6, 1498, 10, 474, 31067, 8, 1323, 10, 5181, 7231, 4, 20, 3091, 40, 28, 17317, 8, 15610, 37174, 137, 349, 515, 4, 140, 15908, 6, 5, 394, 17, 27, 29, 2660, 8023, 1540, 6, 40, 1719, 5, 3044, 1042, 4, 20, 517, 606, 25, 140, 15994, 39, 4724, 13, 5, 247, 7, 13490, 190, 25, 1131, 5197, 1693, 1379, 59, 5, 801, 12114, 9, 608, 98, 4, 20, 23387, 14414, 744, 5831, 1595, 727, 6, 151, 42, 186, 4, 140, 16, 67, 1462, 278, 15, 1826, 5, 3932, 17, 27, 29, 830, 632, 8825, 11, 5420, 6, 190, 25, 369, 1961, 503, 32, 3282, 1379, 59, 1078, 4, 20, 5420, 443, 34, 450, 41, 20646, 11, 34377, 1469, 19473, 1200, 11, 485, 360, 4, 140, 34, 67, 57, 35188, 7, 6654, 39, 8442, 10881, 6, 39, 2270, 5448, 9, 9809, 19, 2732, 8, 16687, 39, 1579, 4, 20, 394, 17, 27, 29, 637, 165, 34, 341, 5, 1061, 7, 33656, 414, 31, 10703, 61, 16, 341, 7, 1004, 66, 39, 1983, 4, 20, 394, 34, 314, 5, 33548, 9, 5, 735, 446, 81, 5, 375, 367, 688, 7, 946, 29578, 781, 1061, 11, 7021, 982, 101, 2605, 8, 2293, 6, 147, 4583, 33, 2343, 123, 12564, 43019, 1557, 6615, 2101, 15478, 4, 20, 1061, 33, 2128, 56, 5, 619, 9, 10, 2669, 6, 1498, 19, 1656, 12, 995, 930, 4, 140, 17, 27, 29, 9273, 236, 123, 7, 28, 450, 25, 7921, 7, 13490, 5, 247, 150, 1574, 1920, 13, 1095, 12, 415, 12, 8361, 3365, 14, 489, 5, 866, 2572, 10001, 4, 3322, 42, 353, 6, 5, 28683, 1351, 703, 10, 65, 12, 4530, 6859, 6593, 44, 48, 4310, 8814, 1644, 17, 46, 12335, 5, 394, 17, 27, 29, 4724, 7, 11604, 1459, 5, 866, 4, 20, 1514, 4633, 19, 140, 17, 27, 29, 23157, 14, 37, 40, 44, 48, 19746, 730, 372, 456, 4, 17, 46, 20, 394, 17, 27, 29, 1433, 3610, 8023, 3078, 376, 7, 10, 8463, 11, 494, 4, 83, 1904, 494, 316, 11137, 19, 3932, 10721, 625, 261, 368, 21882, 1614, 16475, 21, 18976, 6, 25, 58, 10, 1763, 9, 1061, 14, 353, 7, 28, 27724, 30, 78, 6429, 12305, 140, 4, 1648, 19, 11, 12, 5970, 8023, 1061, 16900, 6, 5, 394, 34, 4925, 6295, 8023, 17582, 2446, 7, 10, 2232, 650, 12, 7254, 368, 2513, 4, 140, 17, 27, 29, 559, 3563, 13968, 66, 33161, 15478, 11, 587, 8, 34, 10, 68, 30024, 153, 1055, 12, 261, 12, 4539, 483, 81, 15478, 8, 5, 1557, 1643, 4, 15478, 34, 11370, 160, 11, 12, 5970, 24556, 13844, 148, 5, 23387, 14414, 6, 1386, 608, 804, 1061, 31, 39, 8951, 184, 4, 7812, 12349, 40, 240, 7, 8253, 68, 31663, 6, 4697, 7, 2725, 5, 3160, 11137, 4, 83, 881, 2725, 1942, 7, 5, 188, 3123, 11137, 40, 240, 7, 492, 68, 5714, 6, 151, 4, 20, 418, 40, 213, 7, 140, 15908, 6, 10, 2660, 8023, 1540, 9, 5, 140, 637, 6, 1172, 496, 1674, 6, 8, 194, 1799, 4]\n"
     ]
    }
   ],
   "source": [
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Articles(\"../data/final-data/debugdata/train_basic.json\")\n",
    "val_data = Articles(\"../data/final-data/debugdata/eval_basic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_data)):\n",
    "    train_data.examples[idx]['text'] = tokenizer.tokenize(train_data.examples[idx]['text'])\n",
    "    if len(train_data.examples[idx]['text']) > 512:\n",
    "        train_data.examples[idx]['text'] = train_data.examples[idx]['text'][:512]\n",
    "    train_data.examples[idx]['text'] = tokenizer.encode(\n",
    "                        train_data.examples[idx]['text'],           \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 512)\n",
    "    train_data.examples[idx]['model_publication'] = 1 if train_data.examples[idx]['model_publication'] == 'target' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create batches with positive samples in first half and negative examples in second half\n",
    "class BatchSamplerWithNegativeSamples(torch.utils.data.Sampler):\n",
    "    def __init__(self, pos_sampler, neg_sampler, batch_size, items):\n",
    "        self._pos_sampler = pos_sampler\n",
    "        self._neg_sampler = neg_sampler\n",
    "        self._items = items\n",
    "        assert batch_size % 2 == 0, 'Batch size must be divisible by two for negative samples.'\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        batch, neg_batch = [], []\n",
    "        neg_sampler = iter(self._neg_sampler)\n",
    "        for pos_idx in self._pos_sampler:\n",
    "            batch.append(pos_idx)\n",
    "            neg_idx = pos_idx\n",
    "            # keep sampling until we get a true negative sample\n",
    "            while self._items[neg_idx] == self._items[pos_idx]:\n",
    "                try:\n",
    "                    neg_idx = next(neg_sampler)\n",
    "                except StopIteration:\n",
    "                    neg_sampler = iter(self._neg_sampler)\n",
    "                    neg_idx = next(neg_sampler)\n",
    "            neg_batch.append(neg_idx)\n",
    "            if len(batch) == self._batch_size // 2:\n",
    "                batch.extend(neg_batch)\n",
    "                yield batch\n",
    "                batch, neg_batch = [], []\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._pos_sampler) // self._batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to return necessary data for dataloader to pass into model\n",
    "def collate_fn(examples):\n",
    "    words = []\n",
    "    articles = []\n",
    "    labels = []\n",
    "    publications = []\n",
    "    for example in examples:\n",
    "        words.append(example['text'])\n",
    "        articles.append(example['url'])\n",
    "        labels.append(example['model_publication'])\n",
    "        publications.append(example['publication'])\n",
    "    num_words = [len(x) for x in words]\n",
    "    words = np.concatenate(words, axis=0)\n",
    "    word_attributes = torch.tensor(words, dtype=torch.long)\n",
    "    articles = torch.tensor(articles, dtype=torch.long)\n",
    "    num_words.insert(0,0)\n",
    "    num_words.pop(-1)\n",
    "    attribute_offsets = torch.tensor(np.cumsum(num_words), dtype=torch.long)\n",
    "    publications = torch.tensor(publications, dtype=torch.long)\n",
    "    real_labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return publications, articles, word_attributes, attribute_offsets, real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 433/433 [00:00<00:00, 290581.38B/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 440473133/440473133 [01:37<00:00, 4499883.22B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "import arguments.train_arguments as arguments\n",
    "from data_processing.articles import Articles\n",
    "from models.models import InnerProduct\n",
    "import data_processing.dictionaries as dictionary\n",
    "import sampling.sampler_util as sampler_util\n",
    "import training.eval_util as eval_util\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
