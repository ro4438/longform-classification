{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionaries loaded.\n",
      "Data loaded.\n",
      "Data tokenized.\n",
      "Model Loaded.\n",
      "Model Embeddings and Bias Saved!\n",
      "Publication Embeddings Saved!\n",
      "Demo Articles Saved!\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models.models import InnerProduct\n",
    "import pandas as pd\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "from data_processing.articles import Articles\n",
    "from models.models import InnerProduct\n",
    "import data_processing.dictionaries as dictionary\n",
    "import sampling.sampler_util as sampler_util\n",
    "import training.eval_util as eval_util\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\n",
    "def expand_path(string):\n",
    "    return Path(os.path.expandvars(string))\n",
    "#get arguments for script and parse\n",
    "parser = argparse.ArgumentParser(description='Train model on article data and test evaluation')\n",
    "parser.add_argument('--model_path',\n",
    "                    type=expand_path,\n",
    "                    help=\"This is required to load model.\")\n",
    "\n",
    "parser.add_argument('--dict_dir',\n",
    "                    type=expand_path,\n",
    "                    help=\"This is required to load dictionaries\")\n",
    "\n",
    "parser.add_argument('--dataset_path',\n",
    "                    type=expand_path,\n",
    "                    required=True,\n",
    "                    help='Path to data to be ranked.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "dict_dir = Path(args.dict_dir)\n",
    "final_word_ids,final_url_ids, final_publication_ids = dictionary.load_dictionaries(dict_dir)\n",
    "print(\"Dictionaries loaded.\")\n",
    "\n",
    "data_path = Path(args.dataset_path)\n",
    "dataset = Articles(data_path)\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "dataset.tokenize()\n",
    "print(\"Data tokenized.\")\n",
    "word_counter = collections.Counter()\n",
    "for example in dataset.examples:\n",
    "    word_counter.update(example['text'])\n",
    "\n",
    "unique_words = [word for word in word_counter.keys()]\n",
    "len(set(unique_words))\n",
    "\n",
    "abs_model_path = Path(args.model_path)\n",
    "kwargs = dict(n_publications=len(final_publication_ids),\n",
    "              n_articles=len(final_url_ids),\n",
    "              n_attributes=len(final_word_ids),\n",
    "              emb_size=100,\n",
    "              sparse=False,\n",
    "              use_article_emb=False,\n",
    "              mode='mean')\n",
    "model = InnerProduct(**kwargs)\n",
    "model.load_state_dict(torch.load(abs_model_path))\n",
    "print(\"Model Loaded.\")\n",
    "\n",
    "publication_emb = model.publication_embeddings.weight.data[0].cpu().numpy()\n",
    "publication_bias = model.publication_bias.weight.data[0].cpu().numpy()\n",
    "word_emb = model.attribute_emb_sum.weight.data.cpu().numpy()\n",
    "word_bias = model.attribute_bias_sum.weight.data.cpu().numpy()\n",
    "\n",
    "unique_words = list(set(unique_words))\n",
    "word_emb_and_bias_dict = {}\n",
    "for word in unique_words:\n",
    "    if final_word_ids.get(word, 'None') != 'None':\n",
    "        idx = final_word_ids.get(word, 'None')\n",
    "        current_emb = list(word_emb[idx].astype(float))\n",
    "        current_bias = list(word_bias[idx].astype(float))[0]\n",
    "        current_short_dict = {'embedding':current_emb, 'bias':current_bias}\n",
    "        word_emb_and_bias_dict[word] = current_short_dict\n",
    "\n",
    "with open(\"word_to_emb+bias_dict.json\", 'w') as file:\n",
    "    json.dump(word_emb_and_bias_dict, file, separators=(',', ':'))\n",
    "print(\"Model Embeddings and Bias Saved!\")\n",
    "\n",
    "pub_dict = {\"embedding\": list(publication_emb.astype(float)), \"bias\": list(publication_bias.astype(float))[0]}\n",
    "with open(\"pub_emb+bias.json\", 'w') as file:\n",
    "    json.dump(pub_dict, file, separators=(',', ':'))\n",
    "print(\"Publication Embeddings Saved!\")\n",
    "\n",
    "df = json_normalize(dataset)\n",
    "df.drop(columns=['link', 'model_publication'], inplace=True)\n",
    "df = df[df.text.apply(lambda x: len(x) > 400)]\n",
    "df.to_json(\"select_demo_articles.json\", orient='records')\n",
    "print(\"Demo Articles Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955700039863586"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(publication_bias.astype(float))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
