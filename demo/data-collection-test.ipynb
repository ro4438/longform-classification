{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionaries loaded.\n",
      "Data loaded.\n",
      "Data tokenized.\n",
      "Model Loaded.\n",
      "Model Embeddings and Bias Saved!\n",
      "Publication Embeddings Saved!\n",
      "Demo Articles Saved!\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models.models import InnerProduct\n",
    "import pandas as pd\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "from data_processing.articles import Articles\n",
    "from models.models import InnerProduct\n",
    "import data_processing.dictionaries as dictionary\n",
    "import sampling.sampler_util as sampler_util\n",
    "import training.eval_util as eval_util\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\n",
    "def expand_path(string):\n",
    "    return Path(os.path.expandvars(string))\n",
    "#get arguments for script and parse\n",
    "parser = argparse.ArgumentParser(description='Train model on article data and test evaluation')\n",
    "parser.add_argument('--model_path',\n",
    "                    type=expand_path,\n",
    "                    help=\"This is required to load model.\")\n",
    "\n",
    "parser.add_argument('--dict_dir',\n",
    "                    type=expand_path,\n",
    "                    help=\"This is required to load dictionaries\")\n",
    "\n",
    "parser.add_argument('--dataset_path',\n",
    "                    type=expand_path,\n",
    "                    required=True,\n",
    "                    help='Path to data to be ranked.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "dict_dir = Path(args.dict_dir)\n",
    "final_word_ids,final_url_ids, final_publication_ids = dictionary.load_dictionaries(dict_dir)\n",
    "print(\"Dictionaries loaded.\")\n",
    "\n",
    "data_path = Path(args.dataset_path)\n",
    "dataset = Articles(data_path)\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "dataset.tokenize()\n",
    "print(\"Data tokenized.\")\n",
    "word_counter = collections.Counter()\n",
    "for example in dataset.examples:\n",
    "    word_counter.update(example['text'])\n",
    "\n",
    "unique_words = [word for word in word_counter.keys()]\n",
    "len(set(unique_words))\n",
    "\n",
    "abs_model_path = Path(args.model_path)\n",
    "kwargs = dict(n_publications=len(final_publication_ids),\n",
    "              n_articles=len(final_url_ids),\n",
    "              n_attributes=len(final_word_ids),\n",
    "              emb_size=100,\n",
    "              sparse=False,\n",
    "              use_article_emb=False,\n",
    "              mode='mean')\n",
    "model = InnerProduct(**kwargs)\n",
    "model.load_state_dict(torch.load(abs_model_path))\n",
    "print(\"Model Loaded.\")\n",
    "\n",
    "publication_emb = model.publication_embeddings.weight.data[0].cpu().numpy()\n",
    "publication_bias = model.publication_bias.weight.data[0].cpu().numpy()\n",
    "word_emb = model.attribute_emb_sum.weight.data.cpu().numpy()\n",
    "word_bias = model.attribute_bias_sum.weight.data.cpu().numpy()\n",
    "\n",
    "unique_words = list(set(unique_words))\n",
    "word_emb_and_bias_dict = {}\n",
    "for word in unique_words:\n",
    "    if final_word_ids.get(word, 'None') != 'None':\n",
    "        idx = final_word_ids.get(word, 'None')\n",
    "        current_emb = list(word_emb[idx].astype(float))\n",
    "        current_bias = list(word_bias[idx].astype(float))[0]\n",
    "        current_short_dict = {'embedding':current_emb, 'bias':current_bias}\n",
    "        word_emb_and_bias_dict[word] = current_short_dict\n",
    "\n",
    "with open(\"word_to_emb+bias_dict.json\", 'w') as file:\n",
    "    json.dump(word_emb_and_bias_dict, file, separators=(',', ':'))\n",
    "print(\"Model Embeddings and Bias Saved!\")\n",
    "\n",
    "pub_dict = {\"embedding\": list(publication_emb.astype(float)), \"bias\": list(publication_bias.astype(float))[0]}\n",
    "with open(\"pub_emb+bias.json\", 'w') as file:\n",
    "    json.dump(pub_dict, file, separators=(',', ':'))\n",
    "print(\"Publication Embeddings Saved!\")\n",
    "\n",
    "df = json_normalize(dataset)\n",
    "df.drop(columns=['link', 'model_publication'], inplace=True)\n",
    "df = df[df.text.apply(lambda x: len(x) > 400)]\n",
    "df.to_json(\"select_demo_articles.json\", orient='records')\n",
    "print(\"Demo Articles Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955700039863586"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(publication_bias.astype(float))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionaries loaded.\n",
      "Data loaded.\n",
      "Model Loaded.\n",
      "Final:  8234\n",
      "Filtered, Mapped Data saved to \\users\\rohan\\news-classification\\Data\\feed_ranks\\data\\mapped-data directory\n",
      "-------------------\n",
      "Article-Word Matrix Saved\n",
      "Word Embeddings Saved\n",
      "Word Biases Saved\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models.models import InnerProduct\n",
    "import pandas as pd\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "from data_processing.articles import Articles\n",
    "from models.models import InnerProduct\n",
    "import data_processing.dictionaries as dictionary\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "from scipy import sparse\n",
    "\n",
    "dict_dir = Path(\"../../data/dictionaries\")\n",
    "final_word_ids,final_url_ids, final_publication_ids = dictionary.load_dictionaries(dict_dir)\n",
    "print(\"Dictionaries loaded.\")\n",
    "\n",
    "data_path = Path('../../data/UI_feeds/scrape_data/browser-rss-articles-info.json')\n",
    "dataset = Articles(data_path)\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "abs_model_path = Path(\"../../data/gridsearch_results/2020-05-25/optimizer_type=RMS_use_all_words=False_emb_size=100_learning_rate=0.0001_word_embedding_type=mean/model/mean-inner-product-model.pt\")\n",
    "kwargs = dict(n_publications=len(final_publication_ids),\n",
    "              n_articles=len(final_url_ids),\n",
    "              n_attributes=len(final_word_ids),\n",
    "              emb_size=100,\n",
    "              sparse=False,\n",
    "              use_article_emb=False,\n",
    "              mode='mean')\n",
    "model = InnerProduct(**kwargs)\n",
    "model.load_state_dict(torch.load(abs_model_path))\n",
    "print(\"Model Loaded.\")\n",
    "\n",
    "dataset.tokenize()\n",
    "proper_data = dataset.map_items(final_word_ids,\n",
    "                    final_url_ids,\n",
    "                    final_publication_ids,\n",
    "                    filter=True,\n",
    "                    min_length=400)\n",
    "\n",
    "data_path = Path(\"/users/rohan/news-classification/Data/feed_ranks/data\")\n",
    "if not data_path.is_dir():\n",
    "    data_path.mkdir()\n",
    "mapped_data_path = data_path / \"mapped-data\"\n",
    "if not mapped_data_path.is_dir():\n",
    "    mapped_data_path.mkdir()\n",
    "train_mapped_path = mapped_data_path / \"mapped_dataset.json\"\n",
    "with open(train_mapped_path, \"w\") as file:\n",
    "    json.dump(proper_data, file)\n",
    "raw_data = Articles(train_mapped_path)\n",
    "print(\"Final: \", len(raw_data))\n",
    "print(f\"Filtered, Mapped Data saved to {mapped_data_path} directory\")\n",
    "print(\"-------------------\")\n",
    "\n",
    "word_articles = csr_matrix((len(raw_data), len(final_word_ids)), dtype=np.float32).toarray()\n",
    "\n",
    "for idx, item in enumerate(raw_data.examples):\n",
    "    item['text'] = list(set(item['text']))\n",
    "    for entry in item['text']:\n",
    "        word_articles[idx][entry] = 1\n",
    "\n",
    "publication_emb = model.publication_embeddings.weight.data[0].cpu().numpy()\n",
    "publication_bias = model.publication_bias.weight.data[0].cpu().numpy()\n",
    "word_emb = model.attribute_emb_sum.weight.data.cpu().numpy()\n",
    "word_bias = model.attribute_bias_sum.weight.data.cpu().numpy()\n",
    "\n",
    "np.save(\"word_articles.npy\", word_articles)\n",
    "print(\"Article-Word Matrix Saved\")\n",
    "\n",
    "np.save(\"word_emb.npy\", word_emb)\n",
    "print(\"Word Embeddings Saved\")\n",
    "\n",
    "np.save(\"word_bias.npy\", word_bias)\n",
    "print(\"Word Biases Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99557], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publication_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings = (csr_matrix(word_articles) * csr_matrix(word_emb)).toarray()\n",
    "\n",
    "emb_times_publication = np.dot(article_embeddings, publication_emb.reshape(100,1))\n",
    "\n",
    "article_bias = np.dot(word_articles, word_bias)\n",
    "\n",
    "product_with_bias = emb_times_publication + article_bias\n",
    "\n",
    "word_counts = word_articles.sum(axis=1).reshape(word_articles.shape[0], 1)\n",
    "\n",
    "final_logits = np.divide(product_with_bias, word_counts) + float(publication_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(final_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "publication_emb = np.asarray([1.0440499, 1.0030843, 1.0340449, 0.992087, 1.0509816, 1.0315005, -1.0493797, -1.0198538, 0.9712321, -1.026394, \n",
    "            -0.9687971, 1.0592866, -1.0200703, -1.0423145, 0.9929519, 1.0220934, 1.021279, -1.0265925, 0.9601833, 0.9763889, \n",
    "            1.0109168, -0.9728226, 0.97199583, -1.0237931, -0.9996001, 0.9932069, 0.97966635, -0.98893607, -0.9876815, -0.98812914, \n",
    "            -0.9625895, 0.99879754, 0.9876508, -0.9581506, -0.95436096, -0.9601925, -1.0134513, -0.98763955, 0.98665, -1.0140482, \n",
    "            1.004904, 0.9894275, -1.0044671, -0.9839679, -0.97082543, -0.9798079, 0.9926766, -0.97317344, 0.9797, -0.97642475, \n",
    "            -0.99420726, -0.9972062, -1.0104703, 1.0575777, 0.9957696, -1.0413874, -1.0056863, -1.0151271, -0.99969465, 0.97463423, \n",
    "            -0.98398715, -1.0211866, -1.0128828, -1.0024365, -0.9800189, 1.0457181, 1.0155835, -1.036794, -1.013707, -1.0498024, \n",
    "            -1.0252678, -1.0388161, -0.97501564, 0.97687274, 0.97906756, 1.0536852, 1.0590494, -0.96917725, 1.0247189, -0.9818878, \n",
    "            -1.0417286, -1.0204054, -1.0285249, -1.0329671, 0.9705739, 0.96375024, 0.9891868, 0.9892464, 1.039075, 1.0042666,\n",
    "            0.9786834, 1.0199072, 0.98080486, 0.9698635, -0.99322844, -0.95841753, -0.99150276, 0.97394156, 0.9976019, -1.0375009], dtype=np.float32)\n",
    "\n",
    "publication_bias = 0.99557\n",
    "word_articles = np.load('word_articles.npy')\n",
    "word_emb = np.load('word_emb.npy')\n",
    "word_bias = np.load('word_bias.npy')\n",
    "\n",
    "print(\"Data Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 27.5 GiB for an array with shape (8234, 448718) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ace43847e953>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mword_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpublication_emb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mword_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mbroadcasted_words_per_article\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mword_articles\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword_logits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0msorted_word_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbroadcasted_words_per_article\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mtime2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 27.5 GiB for an array with shape (8234, 448718) and data type int64"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time1 = time.time()\n",
    "article_embeddings = np.dot(word_articles, word_emb)\n",
    "\n",
    "emb_times_publication = np.dot(article_embeddings, publication_emb.reshape(100,1))\n",
    "\n",
    "article_bias = np.dot(word_articles, word_bias)\n",
    "\n",
    "product_with_bias = emb_times_publication + article_bias\n",
    "\n",
    "word_counts = word_articles.sum(axis=1).reshape(word_articles.shape[0], 1)\n",
    "\n",
    "final_logits = np.divide(product_with_bias, word_counts) + float(publication_bias)\n",
    "\n",
    "word_logits = np.dot(word_emb, publication_emb.reshape(100,1)) + word_bias\n",
    "broadcasted_words_per_article =word_articles * word_logits.T\n",
    "sorted_word_indices = broadcasted_words_per_article.argsort(axis=1)\n",
    "\n",
    "time2 = time.time()\n",
    "\n",
    "print(time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8234, 448718)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_words_per_article.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448718"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_word_ids = {v: k for k, v in final_word_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users/rohan/news-classification/data/dictionaries/reversed_word_ids.json', \"w\") as file:\n",
    "    json.dump(reversed_word_ids, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
