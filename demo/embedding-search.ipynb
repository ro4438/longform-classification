{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models.models import InnerProduct\n",
    "import pandas as pd\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "from data_processing.articles import Articles\n",
    "from models.models import InnerProduct\n",
    "import data_processing.dictionaries as dictionary\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "from scipy import sparse\n",
    "import boto3\n",
    "\n",
    "dict_dir = Path(\"../../data/BERT/dictionaries\")\n",
    "final_word_ids,final_url_ids, final_publication_ids = dictionary.load_dictionaries(dict_dir)\n",
    "print(\"Dictionaries loaded.\")\n",
    "\n",
    "with open('/users/rohan/news-classification/data/BERT/dictionaries/reversed_word_ids.json', \"r\") as file:\n",
    "    id_to_word = json.load(file)\n",
    "\n",
    "data_path = Path('../../data/final-data/train.json')\n",
    "dataset = Articles(data_path)\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "dataset.tokenize()\n",
    "proper_data = dataset.map_items(final_word_ids,\n",
    "                    final_url_ids,\n",
    "                    final_publication_ids,\n",
    "                    filter=True,\n",
    "                    min_length=200)\n",
    "\n",
    "data_path = Path(\"/users/rohan/news-classification/Data/embedding-search/data\")\n",
    "if not data_path.is_dir():\n",
    "    data_path.mkdir()\n",
    "mapped_data_path = data_path / \"mapped-data\"\n",
    "if not mapped_data_path.is_dir():\n",
    "    mapped_data_path.mkdir()\n",
    "train_mapped_path = mapped_data_path / \"mapped_dataset.json\"\n",
    "with open(train_mapped_path, \"w\") as file:\n",
    "    json.dump(proper_data, file)\n",
    "raw_data = Articles(train_mapped_path)\n",
    "print(\"Final: \", len(raw_data))\n",
    "print(f\"Filtered, Mapped Data saved to {mapped_data_path} directory\")\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded.\n"
     ]
    }
   ],
   "source": [
    "abs_model_path = Path(\"../../data/gridsearch_results/2020-05-25/optimizer_type=RMS_use_all_words=False_emb_size=100_learning_rate=0.0001_word_embedding_type=mean/model/mean-inner-product-model.pt\")\n",
    "kwargs = dict(n_publications=len(final_publication_ids),\n",
    "              n_articles=len(final_url_ids),\n",
    "              n_attributes=len(final_word_ids),\n",
    "              emb_size=100,\n",
    "              sparse=False,\n",
    "              use_article_emb=False,\n",
    "              mode='mean')\n",
    "model = InnerProduct(**kwargs)\n",
    "model.load_state_dict(torch.load(abs_model_path))\n",
    "print(\"Model Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "cols = []\n",
    "for idx, item in enumerate(raw_data.examples):\n",
    "    word_ids = list(set(item['text']))\n",
    "    number_of_words = np.arange(len(word_ids))\n",
    "    rows.append(np.array(np.ones_like(number_of_words) * idx))\n",
    "    cols.append(np.array(word_ids, dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rows = np.concatenate(rows, axis=None)\n",
    "final_cols = np.concatenate(cols, axis=None)\n",
    "final_data = np.ones_like(final_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_articles = csr_matrix((final_data, (final_rows, final_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Created!\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix Created!\")\n",
    "\n",
    "publication_emb = model.publication_embeddings.weight.data[0].cpu().numpy()\n",
    "publication_bias = model.publication_bias.weight.data[0].cpu().numpy()\n",
    "word_emb = model.attribute_emb_sum.weight.data.cpu().numpy()\n",
    "word_bias = model.attribute_bias_sum.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0440499 ,  1.0030843 ,  1.0340449 ,  0.992087  ,  1.0509816 ,\n",
       "        1.0315005 , -1.0493797 , -1.0198538 ,  0.9712321 , -1.026394  ,\n",
       "       -0.9687971 ,  1.0592866 , -1.0200703 , -1.0423145 ,  0.9929519 ,\n",
       "        1.0220934 ,  1.021279  , -1.0265925 ,  0.9601833 ,  0.9763889 ,\n",
       "        1.0109168 , -0.9728226 ,  0.97199583, -1.0237931 , -0.9996001 ,\n",
       "        0.9932069 ,  0.97966635, -0.98893607, -0.9876815 , -0.98812914,\n",
       "       -0.9625895 ,  0.99879754,  0.9876508 , -0.9581506 , -0.95436096,\n",
       "       -0.9601925 , -1.0134513 , -0.98763955,  0.98665   , -1.0140482 ,\n",
       "        1.004904  ,  0.9894275 , -1.0044671 , -0.9839679 , -0.97082543,\n",
       "       -0.9798079 ,  0.9926766 , -0.97317344,  0.9797    , -0.97642475,\n",
       "       -0.99420726, -0.9972062 , -1.0104703 ,  1.0575777 ,  0.9957696 ,\n",
       "       -1.0413874 , -1.0056863 , -1.0151271 , -0.99969465,  0.97463423,\n",
       "       -0.98398715, -1.0211866 , -1.0128828 , -1.0024365 , -0.9800189 ,\n",
       "        1.0457181 ,  1.0155835 , -1.036794  , -1.013707  , -1.0498024 ,\n",
       "       -1.0252678 , -1.0388161 , -0.97501564,  0.97687274,  0.97906756,\n",
       "        1.0536852 ,  1.0590494 , -0.96917725,  1.0247189 , -0.9818878 ,\n",
       "       -1.0417286 , -1.0204054 , -1.0285249 , -1.0329671 ,  0.9705739 ,\n",
       "        0.96375024,  0.9891868 ,  0.9892464 ,  1.039075  ,  1.0042666 ,\n",
       "        0.9786834 ,  1.0199072 ,  0.98080486,  0.9698635 , -0.99322844,\n",
       "       -0.95841753, -0.99150276,  0.97394156,  0.9976019 , -1.0375009 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publication_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 97058  74350   3422  13576  13336  17505   9678   6039   5244   8132\n",
      " 103482   4337  10204   2768  59956  69910  62884   3486   5542  16999\n",
      "   2667  31863  30074  46135 131804  19594   8315  18405  11226   4612\n",
      "  12533  76975   5295   1064  83067   3398   9118 201038   2666  28494\n",
      " 188036  20795  70641   1274   4016  73256   6429   4924   3400  11513\n",
      "  16931   9608   3346  34346  10397  50922  18877  12284  15644  10817\n",
      "    553  54456  11266   7501  61218  24594  94836   3393   2591  10464\n",
      "  21082    251  21234  15394  17625  25837   3392   2602   4326  40904\n",
      "  34602  61136   4243   3602  28584  18602   8292   8163   2352   1436\n",
      "  13367  19263    903   9119  10078     26  32943   8640  16952    196]\n",
      "[  1065  30817  33846  15394  67162  30163  61218  22415  61135 201038\n",
      "  99425   1948 188036   3392   3251  16999  18877   5542   2666  15644\n",
      "  18405  17832  21121  88959  28494   8063   4160   8315   1469   4016\n",
      "   5295  73204  11226   3422  13576   8512  16931  50922  10817   4924\n",
      "  13717  34346   1064   6039   4096   2885  97058   3346  12155   3398\n",
      "   9118  69910  46135  83067  10204  21234  10464  20795  61136  12284\n",
      "  30074   6429   1436   4243   2667    251 131804    553   9608  11266\n",
      "   8292  94836  34602   7501   3602  24594  25837   2591  28584   2602\n",
      "  73256  18602   9119  13367   4326   2352  21082   3393   8163  17625\n",
      "   3400  19263  40904  10078    903     26  32943  16952   8640    196]\n"
     ]
    }
   ],
   "source": [
    "word_emb_indices = word_emb.argsort(axis=0)\n",
    "print(word_emb_indices[:, 1][-100:])\n",
    "print(word_emb_indices[:, 2][-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_word_emb =  word_emb / np.sqrt(np.square(word_emb).sum(axis=-1)).reshape(448718, 1)\n",
    "print(regularized_word_emb.shape)\n",
    "word_emb_indices = regularized_word_emb.argsort(axis=0)\n",
    "for idx in range(100):\n",
    "    current_column = np.copy(word_emb_indices[:, idx][-150:])\n",
    "    twisted_column = current_column.tolist()[::-1]\n",
    "    top_words = []\n",
    "    for index in twisted_column:\n",
    "        top_words.append(id_to_word[str(index)])\n",
    "    print(idx)\n",
    "    if publication_emb[idx] > 0:\n",
    "        print(\"Positive\")\n",
    "    else:\n",
    "        print(\"Negative\")\n",
    "    print(\"---------------------\")\n",
    "    print(top_words)\n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings = word_articles.dot(word_emb)\n",
    "article_bias = word_articles.dot(word_bias)\n",
    "word_counts = word_articles.sum(axis=1).reshape(word_articles.shape[0], 1)\n",
    "\n",
    "for index in range(47, 100):\n",
    "    altered_emb = np.copy(publication_emb)\n",
    "    altered_emb[index] = 10000\n",
    "    \n",
    "    emb_times_publication = np.dot(article_embeddings, altered_emb.reshape(100,1))\n",
    "\n",
    "    product_with_bias = emb_times_publication + article_bias\n",
    "\n",
    "    final_logits = np.divide(product_with_bias, word_counts) \n",
    "\n",
    "    indices = final_logits.argsort(axis=0)[-75:].reshape(75)\n",
    "    \n",
    "    top_articles = word_articles[indices.tolist()[0]]\n",
    "    \n",
    "    word_logits = np.dot(word_emb, altered_emb.reshape(100,1)) + word_bias\n",
    "\n",
    "    broadcasted_words_per_article = top_articles.toarray() * word_logits.T\n",
    "\n",
    "    sorted_word_indices = broadcasted_words_per_article.argsort(axis=1)\n",
    "\n",
    "    return_articles = []\n",
    "    print(index)\n",
    "    print(\"---------------------\")\n",
    "    return_articles = []\n",
    "    i = 0\n",
    "    for idx in indices.tolist()[0]:\n",
    "        return_articles = []\n",
    "        current_article = raw_data[int(idx)]\n",
    "        current_article['logit'] = float(final_logits[int(idx)])\n",
    "        current_sorted_words = sorted_word_indices[i]\n",
    "        top_words = []\n",
    "        least_words = []\n",
    "        for top_word in current_sorted_words[-10:]:\n",
    "            word = id_to_word[str(top_word)]\n",
    "            top_words.append(word)\n",
    "        for least_word in current_sorted_words[:10]:\n",
    "            word = id_to_word[str(least_word)]\n",
    "            least_words.append(word)\n",
    "        current_article['top_words'] = top_words\n",
    "        current_article['least_words'] = least_words\n",
    "        return_articles.append(current_article)\n",
    "        i += 1\n",
    "        ordered_return_articles = return_articles[::-1]\n",
    "        for article in ordered_return_articles:\n",
    "            print(article['title'])\n",
    "            print(article['link'])\n",
    "            print(article['top_words'])\n",
    "            print(article['least_words'])\n",
    "            print(article['logit'])\n",
    "        print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 13017,  13017,  13017,  32216,  13017,  13017, 110173, 111031,\n",
       "          13017, 100152, 111889,  13017, 111031, 110173,  13017,  32216,\n",
       "          32216, 110173,  13017,  13017,  13017, 111031,  13017, 102570,\n",
       "         111031,  13017,  13017, 109602, 100152, 111806, 100152,  13017,\n",
       "          32216, 100152, 100152, 102570, 110173, 100152,  13017, 111031,\n",
       "          13017,  13017, 111031, 102570, 100152, 111806,  13017, 109602,\n",
       "          13017, 110173, 111031, 110173, 100152,  13017,  13017, 110173,\n",
       "         109602, 102570, 111031,  32216, 109602, 109602, 102570, 106597,\n",
       "         100152,  13017,  13017, 100152, 111031, 109602, 109602, 110173,\n",
       "         109602,  13017,  13017,  13017,  32216, 106597,  13017, 106597,\n",
       "         100152, 110173, 102570, 109602,  13017,  13017,  13017,  13017,\n",
       "          13017,  13017,  13017,  13017,  13017,  13017, 100152, 107710,\n",
       "         100152,  13017,  13017, 111031]], dtype=int64)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked = np.argsort(article_embeddings, axis=0)\n",
    "ranked[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(100):\n",
    "    current_iteration = ranked[:, idx][::-1]\n",
    "    listed_iteration = np.squeeze(current_iteration).tolist()[0]\n",
    "    print(idx)\n",
    "    print(\"-----------------------------\")\n",
    "    for top in range(20):   \n",
    "        print(listed_iteration[top], raw_data[listed_iteration[top]]['title'])\n",
    "        print(article_embeddings[listed_iteration[top], idx])\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../bert-approach\")\n",
    "import pandas as pd\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "from data_processing.articles import Articles\n",
    "from models.models import InnerProduct\n",
    "import data_processing.dictionaries as dictionary\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "from scipy import sparse\n",
    "import boto3\n",
    "\n",
    "dict_dir = Path(\"../../data/BERT/dictionaries\")\n",
    "final_word_ids,final_url_ids, final_publication_ids = dictionary.load_dictionaries(dict_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_word_ids = {v: k for k, v in final_word_ids.items()}\n",
    "with open('/users/rohan/news-classification/data/BERT/dictionaries/reversed_word_ids.json', \"w\") as file:\n",
    "    json.dump(reversed_word_ids, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
