% !TEX root = recommending-interesting-writing.tex
\section{Evaluation}
\label{sec:experiments}

We study the performance of \acrlong{rfs} on a dataset that we collect to assess whether \gls{rfs}, in addition to providing interpretable recommendations to help editors make decisions about articles.

\paragraph{Data collection}

\paragraph{Experimental setup: RankFromSets}

\paragraph{Experimental setup: BERT}

for rankfromsets we searched over 6 embedding sizes [10,25,50, 100, 500, 1000] and 5 batch sizes [500, 1000, 2000, 5000, 10000]. We tried two optimizers, RMS and SGD, both with a momentum of 0.9 and learning rates of [1e-2, 1e-3, 1e-4, 1e-5] for RMS, while learning rates for SGD varied based on batch size and average token count
Breakdown of Data: Train -      Total: 100797            Positive: 18598                 Negative: 82199
Test -      Total: 272448          Positive: 4049                  Negative: 268399
Evaluation -      Total: 272447            Positive: 4039                  Negative: 268348
Both models used the same randomly selected 50k articles from the evaluation set for validation, and the entire test set was used at the end to generate predictions on new data.

\input{fig/training-recall}
\input{table/recall}

\paragraph{Evaluation.} Qualitatively, editors at The Browser preferred the recommendations of our pipeline over their current workflow of a reading list sorted in terms of recency and use the system in production.